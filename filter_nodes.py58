import requests
from bs4 import BeautifulSoup
import os
import time

# GitHub API Token
GITHUB_TOKEN = 'your_github_token_here'  

# 设置请求头
headers = {
    'Authorization': f'token {GITHUB_TOKEN}',
    'Accept': 'application/vnd.github.v3+json',
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) Chrome/91.0.4472.124'
}

# 保存网址的函数
def save_urls(urls, file_path):
    """将网址列表保存到指定文件"""
    os.makedirs(os.path.dirname(file_path), exist_ok=True)
    with open(file_path, 'a', encoding='utf-8') as f:
        for url in urls:
            f.write(f"{url}\n")
    print(f"已保存 {len(urls)} 个网址到 {file_path}")

# 使用 GitHub API 搜索仓库
def search_repositories():
    """通过 API 搜索 GitHub 仓库，获取仓库 URL"""
    search_url = "https://api.github.com/search/repositories?q=stars:>1&sort=stars&per_page=100"
    urls = []
    try:
        response = requests.get(search_url, headers=headers)
        response.raise_for_status()
        data = response.json()
        items = data.get('items', [])
        for item in items:
            repo_url = item['html_url']
            urls.append(repo_url)
        return urls
    except Exception as e:
        print(f"API 搜索仓库时出错: {e}")
        return []

# 使用 GitHub API 搜索代码文件
def search_code_files(keyword):
    """通过 API 搜索包含特定关键词的代码文件"""
    search_url = f"https://api.github.com/search/code?q={keyword}+in:file"
    urls = []
    try:
        response = requests.get(search_url, headers=headers)
        response.raise_for_status()
        data = response.json()
        items = data.get('items', [])
        for item in items:
            repo = item['repository']['full_name']
            path = item['path']
            raw_url = f"https://raw.githubusercontent.com/{repo}/master/{path}"
            urls.append(raw_url)
        return urls
    except Exception as e:
        print(f"API 搜索 {keyword} 时出错: {e}")
        return []

# 使用网页爬虫搜索 GitHub
def crawl_github_web(query):
    """通过网页爬虫搜索 GitHub，提取链接"""
    search_url = f"https://github.com/search?q={query}&type=repositories"
    urls = []
    try:
        response = requests.get(search_url, headers={'User-Agent': headers['User-Agent']}, timeout=10)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, 'html.parser')
        links = soup.find_all('a', href=True)
        for link in links:
            href = link['href']
            if href.startswith('/'):
                full_url = f"https://github.com{href}"
                urls.append(full_url)
        return urls
    except Exception as e:
        print(f"网页搜索 {query} 时出错: {e}")
        return []

# 主函数
def main():
    all_urls = []

    # 1. 通过 API 搜索热门仓库
    print("正在搜索热门仓库...")
    repo_urls = search_repositories()
    all_urls.extend(repo_urls)

    # 2. 通过 API 搜索特定关键词的代码文件
    keywords = ["python", "javascript", "readme"]  # 可扩展关键词
    for keyword in keywords:
        print(f"正在搜索包含 {keyword} 的代码文件...")
        code_urls = search_code_files(keyword)
        all_urls.extend(code_urls)
        time.sleep(2)  # 避免触发 API 限制

    # 3. 通过网页爬虫搜索
    queries = ["open source", "free software"]  # 可扩展查询词
    for query in queries:
        print(f"正在网页搜索 {query}...")
        web_urls = crawl_github_web(query)
        all_urls.extend(web_urls)
        time.sleep(5)  # 避免频繁请求

    # 去重并保存
    unique_urls = list(set(all_urls))
    save_urls(unique_urls, 'data/github_urls.txt')
    print(f"总计收集到 {len(unique_urls)} 个唯一网址")

if __name__ == "__main__":
    main()
