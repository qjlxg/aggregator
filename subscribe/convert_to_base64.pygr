import requests
import base64
import os
import json
import re
import yaml
from urllib.parse import urlparse, parse_qs, unquote
import hashlib
import socket
import time
import concurrent.futures

# --- Proxy Parsing Functions ---
def generate_proxy_fingerprint(proxy_data):
    """ÁîüÊàê‰ª£ÁêÜÁöÑÂîØ‰∏ÄÂìàÂ∏åÊåáÁ∫πÁî®‰∫éÂéªÈáç"""
    parts = [
        proxy_data.get('type', ''),
        str(proxy_data.get('server', '')),
        str(proxy_data.get('port', '')),
        str(proxy_data.get('uuid', '')),
        str(proxy_data.get('password', '')),
        str(proxy_data.get('cipher', '')),
        str(proxy_data.get('network', '')),
        str(proxy_data.get('tls', '')),
        str(proxy_data.get('servername', '')),
        str(proxy_data.get('ws-path', '')),
        str(proxy_data.get('plugin-info', '')),
        str(proxy_data.get('alpn', ''))
    ]
    return hashlib.md5("_".join(parts).encode('utf-8')).hexdigest()

def parse_vmess(vmess_url):
    try:
        json_str = base64.b64decode(vmess_url[8:]).decode('utf-8')
        config = json.loads(json_str)
        proxy = {
            'name': config.get('ps', f"Vmess-{config.get('add')}"),
            'type': 'vmess',
            'server': config.get('add'),
            'port': int(config.get('port')),
            'uuid': config.get('id'),
            'alterId': config.get('aid', 0),
            'cipher': config.get('scy', 'auto'),
            'network': config.get('net', 'tcp'),
            'tls': config.get('tls', '') == 'tls'
        }
        if proxy['tls']:
            proxy['servername'] = config.get('sni', config.get('host', ''))
        if config.get('v', '') == '1':
            proxy['skip-cert-verify'] = True
        return proxy
    except Exception:
        return None

def parse_trojan(trojan_url):
    try:
        parsed = urlparse(trojan_url)
        proxy = {
            'name': unquote(parsed.fragment) or f"Trojan-{parsed.hostname}",
            'type': 'trojan',
            'server': parsed.hostname,
            'port': parsed.port,
            'password': parsed.username,
            'tls': True
        }
        params = parse_qs(parsed.query)
        if params.get('sni'):
            proxy['servername'] = params['sni'][0]
        if params.get('allowInsecure', ['0'])[0] == '1':
            proxy['skip-cert-verify'] = True
        return proxy
    except Exception:
        return None

def parse_shadowsocks(ss_url):
    try:
        encoded_part = ss_url[5:]
        name = "Shadowsocks"
        plugin_info = ""
        if '#' in encoded_part:
            encoded_part, fragment = encoded_part.split('#', 1)
            name = unquote(fragment)
        if '/?plugin=' in encoded_part:
            encoded_part, plugin_info = encoded_part.split('/?plugin=', 1)
            plugin_info = unquote(plugin_info)
        if len(encoded_part) % 4:
            encoded_part += '=' * (4 - len(encoded_part) % 4)
        decoded = base64.urlsafe_b64decode(encoded_part).decode('utf-8', errors='ignore')
        method_password, server_port = decoded.split('@', 1)
        method, password = method_password.split(':', 1)
        server_port_match = re.match(r'^[\w\d\.\-]+\:\d+', server_port)
        if not server_port_match:
            return None
        server, port = server_port_match.group(0).split(':')
        proxy = {
            'name': name,
            'type': 'ss',
            'server': server,
            'port': int(port),
            'cipher': method,
            'password': password
        }
        if plugin_info:
            proxy['plugin-info'] = plugin_info
        return proxy
    except Exception:
        return None

def parse_hysteria2(hy2_url):
    try:
        parsed = urlparse(hy2_url)
        proxy = {
            'name': unquote(parsed.fragment) or f"Hysteria2-{parsed.hostname}",
            'type': 'hysteria2',
            'server': parsed.hostname,
            'port': parsed.port,
            'password': parsed.username,
            'tls': parse_qs(parsed.query).get('security', [''])[0].lower() == 'tls'
        }
        params = parse_qs(parsed.query)
        if params.get('sni'):
            proxy['servername'] = params['sni'][0]
        if params.get('insecure', ['0'])[0] == '1':
            proxy['skip-cert-verify'] = True
        if params.get('fastopen', ['0'])[0] == '1':
            proxy['fast-open'] = True
        if params.get('alpn'):
            proxy['alpn'] = ','.join(params['alpn'])
        return proxy
    except Exception:
        return None

# --- Connectivity Test Function ---
def test_tcp_connectivity(server, port, timeout=0.5, retries=0):
    """Âø´ÈÄüÊµãËØïTCPËøûÊé•ÔºåÂ§±Ë¥•Âç≥ËøîÂõûFalse"""
    try:
        sock = socket.create_connection((server, port), timeout=timeout)
        sock.close()
        return True
    except (socket.timeout, ConnectionRefusedError, OSError):
        return False

# --- Fetch and Decode URLs ---
def fetch_and_decode_urls_to_clash_proxies(urls, enable_connectivity_test=True):
    all_raw_proxies = []
    successful_urls = set()
    exclude_keywords = [
        "cdn.jsdelivr.net", "statically.io", "googletagmanager.com", "www.w3.org",
        "fonts.googleapis.com", "schemes.ogf.org", "clashsub.net", "t.me", "api.w.org",
        "html", "css", "js", "ico", "png", "jpg", "jpeg", "gif", "svg", "webp", "xml", "json", "txt",
        "google-analytics.com", "cloudflare.com/cdn-cgi/", "gstatic.com", "googleapis.com",
        "disqus.com", "gravatar.com", "s.w.org", "amazon.com", "aliyuncs.com", "tencentcos.cn",
        "cdn.bootcss.com", "cdnjs.cloudflare.com", "bit.ly", "tinyurl.com", "cutt.ly",
        "shorturl.at", "surl.li", "suo.yt", "v1.mk", "youtube.com", "facebook.com",
        "twitter.com", "weibo.com", "mail.google.com", "docs.google.com",
        "microsoft.com", "apple.com", "baidu.com", "qq.com",
        ".woff", ".woff2", ".ttf", ".otf", ".eot", ".zip", ".rar", ".7z", ".tar.gz", ".exe", ".dmg", ".apk",
        "/assets/", "/static/", "/images/", "/scripts/", "/styles/", "/fonts/",
        "robots.txt", "sitemap.xml", "favicon.ico", "rss", "atom",
        "/LICENSE", "/README.md", "/CHANGELOG.md", ".git", ".svn", "swagger-ui.html", "openapi.json"
    ]

    for url in urls:
        url = url.strip()
        if not url or any(keyword in url for keyword in exclude_keywords):
            continue

        try:
            response = requests.get(url, timeout=10)
            response.raise_for_status()
            content = response.content

            def try_parse_yaml(text):
                try:
                    data = yaml.safe_load(text)
                    if isinstance(data, dict) and 'proxies' in data:
                        return data['proxies']
                    elif isinstance(data, list) and all(isinstance(item, dict) and 'type' in item for item in data):
                        return data
                    return None
                except yaml.YAMLError:
                    return None

            def try_parse_json_nodes(text):
                try:
                    data = json.loads(text)
                    if isinstance(data, list) and all(isinstance(item, dict) and 'v' in item for item in data):
                        return [parse_vmess(f"vmess://{base64.b64encode(json.dumps(node).encode('utf-8')).decode('utf-8')}") for node in data]
                    return None
                except json.JSONDecodeError:
                    return None

            proxies_from_url = []
            try:
                decoded = content.decode('utf-8')
                yaml_proxies = try_parse_yaml(decoded)
                if yaml_proxies:
                    proxies_from_url.extend(yaml_proxies)
                else:
                    json_proxies = try_parse_json_nodes(decoded)
                    if json_proxies:
                        proxies_from_url.extend([p for p in json_proxies if p])
                    else:
                        try:
                            if len(decoded.strip()) > 0 and len(decoded.strip()) % 4 == 0 and re.fullmatch(r'[A-Za-z0-9+/=]*', decoded.strip()):
                                lines = base64.b64decode(decoded.strip()).decode('utf-8').split('\n')
                            else:
                                lines = decoded.split('\n')
                            for line in lines:
                                line = line.strip()
                                if line.startswith("vmess://"):
                                    p = parse_vmess(line)
                                elif line.startswith("trojan://"):
                                    p = parse_trojan(line)
                                elif line.startswith("ss://"):
                                    p = parse_shadowsocks(line)
                                elif line.startswith("hysteria2://"):
                                    p = parse_hysteria2(line)
                                else:
                                    continue
                                if p:
                                    proxies_from_url.append(p)
                        except (base64.binascii.Error, UnicodeDecodeError):
                            pass
            except UnicodeDecodeError:
                try:
                    lines = base64.b64decode(content.strip()).decode('utf-8').split('\n')
                    yaml_proxies = try_parse_yaml('\n'.join(lines))
                    if yaml_proxies:
                        proxies_from_url.extend(yaml_proxies)
                    else:
                        json_proxies = try_parse_json_nodes('\n'.join(lines))
                        if json_proxies:
                            proxies_from_url.extend([p for p in json_proxies if p])
                        else:
                            for line in lines:
                                line = line.strip()
                                if line.startswith("vmess://"):
                                    p = parse_vmess(line)
                                elif line.startswith("trojan://"):
                                    p = parse_trojan(line)
                                elif line.startswith("ss://"):
                                    p = parse_shadowsocks(line)
                                elif line.startswith("hysteria2://"):
                                    p = parse_hysteria2(line)
                                else:
                                    continue
                                if p:
                                    proxies_from_url.append(p)
                except (base64.binascii.Error, UnicodeDecodeError):
                    pass

            if proxies_from_url:
                all_raw_proxies.extend(proxies_from_url)
                successful_urls.add(url)

        except requests.exceptions.RequestException:
            pass

    # --- ÂéªÈáçÂíåËøûÈÄöÊÄßÊµãËØï ---
    unique_proxies = {}
    for proxy in all_raw_proxies:
        if proxy:
            fingerprint = generate_proxy_fingerprint(proxy)
            unique_proxies[fingerprint] = proxy

    final_proxies = []
    if enable_connectivity_test:
        proxies_to_test = [p for p in unique_proxies.values() if p.get('type') in ['vmess', 'trojan', 'ss']]
        proxies_no_test = [p for p in unique_proxies.values() if p.get('type') not in ['vmess', 'trojan', 'ss']]

        with concurrent.futures.ThreadPoolExecutor(max_workers=50) as executor:
            future_to_proxy = {
                executor.submit(test_tcp_connectivity, p['server'], p['port']): p
                for p in proxies_to_test if p.get('server') and isinstance(p.get('port'), int)
            }
            for future in concurrent.futures.as_completed(future_to_proxy):
                proxy = future_to_proxy[future]
                if future.result():
                    proxy['name'] = f"{proxy['type'].upper()}-{proxy['server']}-{generate_proxy_fingerprint(proxy)[:8]}"
                    final_proxies.append(proxy)

        final_proxies.extend(proxies_no_test)  # Áõ¥Êé•Ê∑ªÂä†‰∏çÊîØÊåÅTCPÊµãËØïÁöÑ‰ª£ÁêÜÔºàÂ¶Çhysteria2Ôºâ
    else:
        for proxy in unique_proxies.values():
            proxy['name'] = f"{proxy['type'].upper()}-{proxy['server']}-{generate_proxy_fingerprint(proxy)[:8]}"
            final_proxies.append(proxy)

    return final_proxies, list(successful_urls)

# --- GitHub API Helpers ---
def get_github_file_content(api_url, token):
    headers = {"Authorization": f"token {token}", "Accept": "application/vnd.github.v3.raw"}
    try:
        response = requests.get(api_url, headers=headers, timeout=10)
        sha = response.headers.get("X-GitHub-Sha") or response.headers.get("ETag", "").strip('"')
        response.raise_for_status()
        return response.text, sha
    except requests.exceptions.RequestException:
        return None, None

def update_github_file_content(repo_contents_api_base, token, file_path, new_content, sha, commit_message):
    url = f"{repo_contents_api_base}/{file_path}"
    headers = {
        "Authorization": f"token {token}",
        "Accept": "application/vnd.github.v3+json",
        "Content-Type": "application/json"
    }
    data = {
        "message": commit_message,
        "content": base64.b64encode(new_content.encode('utf-8')).decode('utf-8'),
        "sha": sha
    }
    try:
        response = requests.put(url, headers=headers, data=json.dumps(data), timeout=10)
        response.raise_for_status()
        return True
    except requests.exceptions.RequestException:
        return False

# --- Main Function ---
def main():
    bot_token = os.environ.get("BOT")
    url_list_repo_api = os.environ.get("URL_LIST_REPO_API")

    try:
        parts = url_list_repo_api.split('/')
        if len(parts) < 8 or parts[2] != 'api.github.com' or parts[3] != 'repos' or parts[6] != 'contents':
            raise ValueError("Invalid URL_LIST_REPO_API format")
        owner, repo_name, file_path = parts[4], parts[5], '/'.join(parts[7:])
        repo_contents_api_base = f"https://api.github.com/repos/{owner}/{repo_name}/contents"
    except (ValueError, IndexError):
        print("Error: Invalid URL_LIST_REPO_API format")
        exit(1)

    if not bot_token or not url_list_repo_api:
        print("Error: Missing BOT or URL_LIST_REPO_API environment variables")
        exit(1)

    url_content, url_file_sha = get_github_file_content(url_list_repo_api, bot_token)
    if not url_content or not url_file_sha:
        print("Error: Could not fetch URL list or SHA")
        exit(1)

    urls = url_content.strip().split('\n')
    enable_connectivity_test = os.environ.get("ENABLE_CONNECTIVITY_TEST", "true").lower() == "true"
    all_proxies, successful_urls = fetch_and_decode_urls_to_clash_proxies(urls, enable_connectivity_test)

    # ÊûÑÂª∫ Clash ÈÖçÁΩÆ
    clash_config = {
        'port': 7890, 'socks-port': 7891, 'redir-port': 7892, 'tproxy-port': 7893, 'mixed-port': 7890,
        'mode': 'rule', 'log-level': 'info', 'allow-lan': True, 'bind-address': '*',
        'external-controller': '127.0.0.1:9090',
        'dns': {
            'enable': True, 'ipv6': False, 'enhanced-mode': 'fake-ip', 'listen': '0.0.0.0:53',
            'default-nameserver': ['114.114.114.114', '8.8.8.8'],
            'nameserver': ['https://dns.google/dns-query', 'tls://dns.google'],
            'fallback': ['tls://1.1.1.1', 'tcp://8.8.4.4', 'https://dns.opendns.com/dns-query'],
            'fallback-filter': {'geoip': True, 'geoip-code': 'CN', 'ipcidr': ['240.0.0.0/4']}
        },
        'proxies': all_proxies,
        'proxy-groups': [
            {'name': 'üöÄ ËäÇÁÇπÈÄâÊã©', 'type': 'select', 'proxies': ['DIRECT'] + [p['name'] for p in all_proxies]},
            {'name': 'üì≤ ÂõΩÂ§ñÂ™í‰Ωì', 'type': 'select', 'proxies': ['üöÄ ËäÇÁÇπÈÄâÊã©', 'DIRECT']},
            {'name': 'ü§ñ AI/ChatGPT', 'type': 'select', 'proxies': ['üöÄ ËäÇÁÇπÈÄâÊã©', 'DIRECT']},
            {'name': 'üåç ÂÖ∂‰ªñÊµÅÈáè', 'type': 'select', 'proxies': ['üöÄ ËäÇÁÇπÈÄâÊã©', 'DIRECT']},
            {'name': 'üêü ÊºèÁΩë‰πãÈ±º', 'type': 'select', 'proxies': ['üöÄ ËäÇÁÇπÈÄâÊã©', 'DIRECT']},
            {'name': 'üõë ÂπøÂëäÊã¶Êà™', 'type': 'select', 'proxies': ['REJECT', 'DIRECT']},
            {'name': 'üî∞ Fallback', 'type': 'fallback', 'proxies': [p['name'] for p in all_proxies],
             'url': 'http://www.google.com/generate_204', 'interval': 300}
        ],
        'rules': [
            'DOMAIN-KEYWORD,openai,ü§ñ AI/ChatGPT',
            'DOMAIN-KEYWORD,google,üì≤ ÂõΩÂ§ñÂ™í‰Ωì',
            'DOMAIN-KEYWORD,youtube,üì≤ ÂõΩÂ§ñÂ™í‰Ωì',
            'DOMAIN-KEYWORD,netflix,üì≤ ÂõΩÂ§ñÂ™í‰Ωì',
            'DOMAIN-KEYWORD,github,üåç ÂÖ∂‰ªñÊµÅÈáè',
            'DOMAIN-SUFFIX,cn,DIRECT',
            'IP-CIDR,172.16.0.0/12,DIRECT,no-resolve',
            'IP-CIDR,192.168.0.0/16,DIRECT,no-resolve',
            'IP-CIDR,10.0.0.0/8,DIRECT,no-resolve',
            'IP-CIDR,127.0.0.1/8,DIRECT,no-resolve',
            'GEOIP,CN,DIRECT,no-resolve',
            'MATCH,üêü ÊºèÁΩë‰πãÈ±º'
        ]
    }

    # ÁîüÊàê YAML Âíå Base64 Êñá‰ª∂
    final_clash_yaml = yaml.dump(clash_config, allow_unicode=True, sort_keys=False, default_flow_style=False, indent=2)
    with open("base64.yaml", "w", encoding="utf-8") as f:
        f.write(final_clash_yaml)
    final_base64_encoded = base64.b64encode(final_clash_yaml.encode('utf-8')).decode('utf-8')
    with open("base64.txt", "w", encoding="utf-8") as f:
        f.write(final_base64_encoded)

    new_url_list_content = "\n".join(sorted(list(set(successful_urls))))
    if new_url_list_content.strip() != url_content.strip():
        update_success = update_github_file_content(
            repo_contents_api_base, bot_token, file_path, new_url_list_content, url_file_sha,
            "feat: Update url.txt with valid subscription links"
        )
        print("url.txt updated" if update_success else "Failed to update url.txt")
    else:
        print("url.txt unchanged")

if __name__ == "__main__":
    main()
