import requests
import base64
import os
import json
import re
import yaml # pip install PyYAML
from urllib.parse import urlparse, parse_qs, unquote
import hashlib # ç”¨äºç”Ÿæˆå“ˆå¸ŒæŒ‡çº¹

# --- Proxy Parsing Functions (Modified to return more detailed info for fingerprinting) ---

def generate_proxy_fingerprint(proxy_data):
    """
    æ ¹æ®ä»£ç†çš„å…³é”®è¿æ¥ä¿¡æ¯ç”Ÿæˆä¸€ä¸ªå”¯ä¸€çš„å“ˆå¸ŒæŒ‡çº¹ã€‚
    è¿™ç”¨äºè¯†åˆ«å’Œå»é‡ç›¸åŒçš„ä»£ç†ï¼Œå³ä½¿å®ƒä»¬çš„åç§°ä¸åŒã€‚
    """
    parts = []
    # å°½å¯èƒ½åŒ…å«æ‰€æœ‰æ ¸å¿ƒè¿æ¥å‚æ•°
    parts.append(proxy_data.get('type', ''))
    parts.append(str(proxy_data.get('server', '')))
    parts.append(str(proxy_data.get('port', '')))
    parts.append(str(proxy_data.get('uuid', '')))
    parts.append(str(proxy_data.get('password', '')))
    parts.append(str(proxy_data.get('cipher', '')))
    parts.append(str(proxy_data.get('network', '')))
    parts.append(str(proxy_data.get('tls', '')))
    parts.append(str(proxy_data.get('servername', '')))
    parts.append(str(proxy_data.get('ws-path', '')))
    # å¯¹äºæ’ä»¶ä¿¡æ¯ï¼Œå¦‚æœå®ƒæ˜¯ä¸€ä¸ªå¤æ‚çš„å­—å…¸ï¼Œå¯èƒ½éœ€è¦å°†å…¶è½¬æ¢ä¸ºå¯å“ˆå¸Œçš„å­—ç¬¦ä¸²
    # ä¸ºäº†ç®€åŒ–å’Œç¡®ä¿å•è¡Œï¼Œæˆ‘ä»¬ä¹‹å‰å·²ç»å°†å…¶å°è¯•æ‰å¹³åŒ–ä¸ºå­—ç¬¦ä¸²
    parts.append(str(proxy_data.get('plugin-info', ''))) 
    parts.append(str(proxy_data.get('alpn', ''))) # for hysteria2

    # ä½¿ç”¨json.dumpsæ¥ç¡®ä¿å­—å…¸å’Œåˆ—è¡¨çš„é¡ºåºä¸€è‡´æ€§ï¼Œä½¿å…¶å¯å“ˆå¸Œ
    # ä½†ç”±äºæˆ‘ä»¬çš„ç›®æ ‡æ˜¯å•è¡Œå­—å…¸ï¼Œé€šå¸¸ä¸ä¼šæœ‰åµŒå¥—çš„dict/listä½œä¸ºå€¼
    # è¿™é‡Œç›´æ¥æ‹¼æ¥å­—ç¬¦ä¸²æ›´é«˜æ•ˆä¸”ç¬¦åˆé¢„æœŸ
    
    unique_string = "_".join(parts)
    return hashlib.md5(unique_string.encode('utf-8')).hexdigest()

def parse_vmess(vmess_url):
    try:
        json_str = base64.b64decode(vmess_url[8:]).decode('utf-8')
        config = json.loads(json_str)
        
        name = config.get('ps', f"Vmess-{config.get('add')}")
        server = config.get('add')
        port = config.get('port')
        uuid = config.get('id')
        alterId = config.get('aid', 0)
        cipher = config.get('scy', 'auto')
        network = config.get('net', 'tcp')
        tls = config.get('tls', '') == 'tls'
        servername = config.get('sni', config.get('host', '')) if tls else ''
        skip_cert_verify = config.get('v', '') == '1' 

        proxy = {
            'name': name, # ä¸´æ—¶åç§°ï¼Œåé¢ä¼šæ ‡å‡†åŒ–
            'type': 'vmess',
            'server': server,
            'port': port,
            'uuid': uuid,
            'alterId': alterId,
            'cipher': cipher,
            'network': network,
            'tls': tls,
        }

        if servername:
            proxy['servername'] = servername
        if skip_cert_verify:
            proxy['skip-cert-verify'] = True
        
        if network == 'ws':
            proxy['ws-path'] = config.get('path', '/')
            if config.get('headers'):
                # å°½é‡å°† headers è½¬æ¢ä¸ºå­—ç¬¦ä¸²ï¼Œä»¥ä¿æŒä»£ç†å­—å…¸æ‰å¹³
                proxy['ws-headers'] = str(config.get('headers')) 

        return proxy
    except Exception as e:
        print(f"è§£æ Vmess é“¾æ¥å¤±è´¥: {vmess_url[:50]}...ï¼ŒåŸå› : {e}")
        return None

def parse_trojan(trojan_url):
    try:
        parsed = urlparse(trojan_url)
        password = parsed.username
        server = parsed.hostname
        port = parsed.port
        name = unquote(parsed.fragment) if parsed.fragment else f"Trojan-{server}"
        
        params = parse_qs(parsed.query)
        tls = True
        skip_cert_verify = params.get('allowInsecure', ['0'])[0] == '1'
        servername = params.get('sni', [server])[0]

        proxy = {
            'name': name,
            'type': 'trojan',
            'server': server,
            'port': port,
            'password': password,
            'tls': tls,
        }
        if servername:
            proxy['servername'] = servername
        if skip_cert_verify:
            proxy['skip-cert-verify'] = True
        
        return proxy
    except Exception as e:
        print(f"è§£æ Trojan é“¾æ¥å¤±è´¥: {trojan_url[:50]}...ï¼ŒåŸå› : {e}")
        return None

def parse_shadowsocks(ss_url):
    try:
        encoded_part = ss_url[5:]
        
        if '#' in encoded_part:
            encoded_part, fragment = encoded_part.split('#', 1)
            name = unquote(fragment)
        else:
            name = "Shadowsocks"

        plugin_info_str = ""
        if '/?plugin=' in encoded_part:
            encoded_part, plugin_info_str = encoded_part.split('/?plugin=', 1)
        
        decoded_str = base64.urlsafe_b64decode(encoded_part + '==').decode('utf-8')
        parts = decoded_str.split('@')
        method_password = parts[0].split(':', 1)
        method = method_password[0]
        password = method_password[1]
        
        server_port = parts[1].split(':')
        server = server_port[0]
        port = int(server_port[1])
        
        proxy = {
            'name': name,
            'type': 'ss',
            'server': server,
            'port': port,
            'cipher': method,
            'password': password,
        }

        if plugin_info_str:
            # å­˜å‚¨ä¸ºå­—ç¬¦ä¸²ï¼Œä»¥é¿å…å¤æ‚åµŒå¥—å¯¼è‡´å¤šè¡Œ
            proxy['plugin-info'] = plugin_info_str 
        
        return proxy
    except Exception as e:
        print(f"è§£æ Shadowsocks é“¾æ¥å¤±è´¥: {ss_url[:50]}...ï¼ŒåŸå› : {e}")
        return None

def parse_hysteria2(hy2_url):
    try:
        parsed = urlparse(hy2_url)
        uuid = parsed.username
        server = parsed.hostname
        port = parsed.port
        name = unquote(parsed.fragment) if parsed.fragment else f"Hysteria2-{server}"

        params = parse_qs(parsed.query)
        tls = params.get('security', [''])[0].lower() == 'tls'
        servername = params.get('sni', [''])[0]
        skip_cert_verify = params.get('insecure', ['0'])[0] == '1'
        fast_open = params.get('fastopen', ['0'])[0] == '1'

        proxy = {
            'name': name,
            'type': 'hysteria2',
            'server': server,
            'port': port,
            'password': uuid,
            'tls': tls,
            'skip-cert-verify': skip_cert_verify,
            'fast-open': fast_open,
        }
        if servername:
            proxy['servername'] = servername
        if params.get('alpn'):
            # å°† alpn åˆ—è¡¨è½¬æ¢ä¸ºé€—å·åˆ†éš”çš„å­—ç¬¦ä¸²
            proxy['alpn'] = ','.join(params['alpn']) 

        return proxy
    except Exception as e:
        print(f"è§£æ Hysteria2 é“¾æ¥å¤±è´¥: {hy2_url[:50]}...ï¼ŒåŸå› : {e}")
        return None

# --- Fetch and Decode URLs (Modified for deduplication and naming) ---
def fetch_and_decode_urls_to_clash_proxies(urls):
    # ä½¿ç”¨å­—å…¸æ¥å­˜å‚¨å”¯ä¸€çš„ä»£ç†ï¼Œkey æ˜¯æŒ‡çº¹ï¼Œvalue æ˜¯ Clash ä»£ç†é…ç½®
    unique_proxies = {} 
    successful_urls = []

    EXCLUDE_KEYWORDS = [
        "cdn.jsdelivr.net", "statically.io", "googletagmanager.com",
        "www.w3.org", "fonts.googleapis.com", "schemes.ogf.org", "clashsub.net", 
        "t.me", "api.w.org",
        "html", "css", "js", "ico", "png", "jpg", "jpeg", "gif", "svg", "webp", "xml", "json", "txt",
        "google-analytics.com", "cloudflare.com/cdn-cgi/", "gstatic.com", "googleapis.com",
        "disqus.com", "gravatar.com", "s.w.org",
        "amazon.com", "aliyuncs.com", "tencentcos.cn",
        "cdn.bootcss.com", "cdnjs.cloudflare.com",
        "bit.ly", "tinyurl.com", "cutt.ly", "shorturl.at", "surl.li", "suo.yt", "v1.mk",
        "youtube.com", "facebook.com", "twitter.com", "weibo.com",
        "mail.google.com", "docs.google.com",
        "microsoft.com", "apple.com", "baidu.com", "qq.com",
        ".woff", ".woff2", ".ttf", ".otf", ".eot",
        ".zip", ".rar", ".7z", ".tar.gz", ".exe", ".dmg", ".apk",
        "/assets/", "/static/", "/images/", "/scripts/", "/styles/", "/fonts/",
        "robots.txt", "sitemap.xml", "favicon.ico",
        "rss", "atom",
        "/LICENSE", "/README.md", "/CHANGELOG.md",
        ".git", ".svn",
        "swagger-ui.html", "openapi.json"
    ]

    for url in urls:
        url = url.strip()
        if not url:
            continue

        if any(keyword in url for keyword in EXCLUDE_KEYWORDS):
            print(f"Skipping non-subscription link (filtered by keyword): {url}")
            continue
            
        print(f"Processing URL: {url}")
        try:
            response = requests.get(url, timeout=20)
            response.raise_for_status()
            content = response.content

            print(f"  --- URL: {url} Downloaded content size: {len(content)} bytes ---")

            decoded_content = ""
            current_proxies = []

            def try_parse_yaml(text):
                try:
                    data = yaml.safe_load(text)
                    if isinstance(data, dict) and 'proxies' in data and isinstance(data['proxies'], list):
                        return data['proxies']
                    elif isinstance(data, list) and all(isinstance(item, dict) and 'type' in item for item in data):
                        return data
                    return None
                except yaml.YAMLError:
                    return None

            def try_parse_json_nodes(text):
                try:
                    data = json.loads(text)
                    if isinstance(data, list) and all(isinstance(item, dict) and 'v' in item for item in data):
                        return [parse_vmess(f"vmess://{base64.b64encode(json.dumps(node).encode('utf-8')).decode('utf-8')}") for node in data]
                    return None
                except json.JSONDecodeError:
                    return None

            try:
                decoded_content = content.decode('utf-8')
                print(f"  --- URL: {url} Successfully decoded to UTF-8 ---")
                
                yaml_proxies = try_parse_yaml(decoded_content)
                if yaml_proxies:
                    current_proxies.extend(yaml_proxies)
                    print(f"  --- URL: {url} Identified as YAML subscription ---")
                else:
                    json_proxies = try_parse_json_nodes(decoded_content)
                    if json_proxies:
                        current_proxies.extend(json_proxies)
                        print(f"  --- URL: {url} Identified as JSON node list ---")
                    else:
                        if len(decoded_content.strip()) > 0 and len(decoded_content.strip()) % 4 == 0 and re.fullmatch(r'[A-Za-z0-9+/=]*', decoded_content.strip()):
                            try:
                                temp_decoded = base64.b64decode(decoded_content.strip()).decode('utf-8')
                                lines = temp_decoded.split('\n')
                                parsed_line_count = 0
                                for line in lines:
                                    line = line.strip()
                                    if line.startswith("vmess://"):
                                        p = parse_vmess(line)
                                        if p: current_proxies.append(p); parsed_line_count += 1
                                    elif line.startswith("trojan://"):
                                        p = parse_trojan(line)
                                        if p: current_proxies.append(p); parsed_line_count += 1
                                    elif line.startswith("ss://"):
                                        p = parse_shadowsocks(line)
                                        if p: current_proxies.append(p); parsed_line_count += 1
                                    elif line.startswith("hysteria2://"):
                                        p = parse_hysteria2(line)
                                        if p: current_proxies.append(p); parsed_line_count += 1
                                if parsed_line_count > 0:
                                    print(f"  --- URL: {url} Base64 decoded and identified {parsed_line_count} proxy nodes ---")
                                else:
                                    print(f"  --- URL: {url} Base64 decoded successfully, but content doesn't match known proxy format.---")
                            except (base64.binascii.Error, UnicodeDecodeError):
                                print(f"  --- URL: {url} Looks like Base64 but failed to decode, treating as plaintext.---")
                        
                        if not current_proxies:
                            lines = decoded_content.split('\n')
                            parsed_line_count = 0
                            for line in lines:
                                line = line.strip()
                                if line.startswith("vmess://"):
                                    p = parse_vmess(line)
                                    if p: current_proxies.append(p); parsed_line_count += 1
                                elif line.startswith("trojan://"):
                                    p = parse_trojan(line)
                                    if p: current_proxies.append(p); parsed_line_count += 1
                                elif line.startswith("ss://"):
                                    p = parse_shadowsocks(line)
                                    if p: current_proxies.append(p); parsed_line_count += 1
                                elif line.startswith("hysteria2://"):
                                    p = parse_hysteria2(line)
                                    if p: current_proxies.append(p); parsed_line_count += 1
                            if parsed_line_count > 0:
                                print(f"  --- URL: {url} Identified as plaintext {parsed_line_count} proxy nodes ---")
                            else:
                                print(f"  --- URL: {url} Content not identified as valid subscription format (UTF-8).---")

            except UnicodeDecodeError:
                print(f"  --- URL: {url} UTF-8 decoding failed, trying Base64 decoding.---")
                try:
                    cleaned_content = content.strip()
                    temp_decoded = base64.b64decode(cleaned_content).decode('utf-8')
                    print(f"  --- URL: {url} Successfully decoded Base64 content to UTF-8 ---")
                    
                    yaml_proxies = try_parse_yaml(temp_decoded)
                    if yaml_proxies:
                        current_proxies.extend(yaml_proxies)
                        print(f"  --- URL: {url} Base64 decoded to YAML subscription ---")
                    else:
                        json_proxies = try_parse_json_nodes(temp_decoded)
                        if json_proxies:
                            current_proxies.extend(json_proxies)
                            print(f"  --- URL: {url} Base64 decoded to JSON node list ---")
                        else:
                            lines = temp_decoded.split('\n')
                            parsed_line_count = 0
                            for line in lines:
                                line = line.strip()
                                if line.startswith("vmess://"):
                                    p = parse_vmess(line)
                                    if p: current_proxies.append(p); parsed_line_count += 1
                                elif line.startswith("trojan://"):
                                    p = parse_trojan(line)
                                    if p: current_proxies.append(p); parsed_line_count += 1
                                elif line.startswith("ss://"):
                                    p = parse_shadowsocks(line)
                                    if p: current_proxies.append(p); parsed_line_count += 1
                                elif line.startswith("hysteria2://"):
                                    p = parse_hysteria2(line)
                                    if p: current_proxies.append(p); parsed_line_count += 1
                            if parsed_line_count > 0:
                                print(f"  --- URL: {url} Base64 decoded to {parsed_line_count} proxy nodes ---")
                            else:
                                print(f"  --- URL: {url} Base64 decoded successfully, but content doesn't match known proxy format.---")

                except (base64.binascii.Error, UnicodeDecodeError) as decode_err:
                    print(f"  --- URL: {url} Base64 decoding or UTF-8 conversion failed: {decode_err} ---")
                    content.decode('latin-1', errors='ignore')
                    print(f"Warning: Could not decode content from {url} to UTF-8, GBK, or Base64. Using latin-1 and ignoring errors.")

            # --- Deduplication and Name Standardization Logic ---
            for proxy_dict in current_proxies:
                if proxy_dict: # ç¡®ä¿ä»£ç†å­—å…¸ä¸æ˜¯None
                    fingerprint = generate_proxy_fingerprint(proxy_dict)
                    if fingerprint not in unique_proxies:
                        # ç”Ÿæˆæ ‡å‡†åŒ–åç§°ï¼šåè®®_æœåŠ¡å™¨_ç«¯å£_æŒ‡çº¹_åºå· (ä¸ºé˜²æ­¢é‡å¤ï¼Œå¯ä»¥åŠ ä¸ªè®¡æ•°å™¨)
                        # è¿™é‡Œæˆ‘ä»¬ç®€åŒ–ä¸º åè®®-æœåŠ¡å™¨-æŒ‡çº¹åå››ä½
                        base_name = f"{proxy_dict.get('type', 'unknown').upper()}-{proxy_dict.get('server', 'unknown')}"
                        # é˜²æ­¢åç§°è¿‡é•¿ï¼Œæˆªæ–­æŒ‡çº¹
                        proxy_dict['name'] = f"{base_name}-{fingerprint[:8]}" 
                        unique_proxies[fingerprint] = proxy_dict
                        print(f"    æ·»åŠ æ–°ä»£ç†: {proxy_dict['name']}")
                    else:
                        print(f"    è·³è¿‡é‡å¤ä»£ç† (æŒ‡çº¹: {fingerprint})")
            
            if current_proxies: # å¦‚æœè¿™ä¸ªURLæˆåŠŸè§£æå‡ºä»£ç†ï¼ˆå³ä½¿æ˜¯é‡å¤çš„ï¼‰
                successful_urls.append(url)

        except requests.exceptions.RequestException as e:
            print(f"Failed to fetch data from URL: {url}, reason: {e}")
        except Exception as e:
            print(f"An unexpected error occurred while processing URL {url}: {e}")

    final_proxies_list = list(unique_proxies.values())
    print(f"Successfully parsed, deduplicated, and aggregated {len(final_proxies_list)} unique proxy nodes.")
    return final_proxies_list, successful_urls

# --- GitHub API Helpers (ä¸å˜) ---
def get_github_file_content(api_url, token):
    headers = {"Authorization": f"token {token}", "Accept": "application/vnd.github.v3.raw"}
    try:
        response = requests.get(api_url, headers=headers, timeout=10)
        response.raise_for_status()
        sha = response.headers.get("X-GitHub-Sha")
        return response.text, sha
    except requests.exceptions.RequestException as e:
        print(f"Error fetching file from GitHub: {e}")
        return None, None

def update_github_file_content(repo_contents_api_base, token, file_path, new_content, sha, commit_message):
    url = f"{repo_contents_api_base}/{file_path}"
    headers = {
        "Authorization": f"token {token}",
        "Accept": "application/vnd.github.v3+json",
        "Content-Type": "application/json"
    }
    data = {
        "message": commit_message,
        "content": base64.b64encode(new_content.encode('utf-8')).decode('utf-8'),
        "sha": sha
    }
    try:
        response = requests.put(url, headers=headers, data=json.dumps(data), timeout=10)
        response.raise_for_status()
        return True
    except requests.exceptions.RequestException as e:
        print(f"Error updating file on GitHub: {e}")
        if response and response.status_code == 409:
            print("Conflict: File content changed on GitHub before commit. Please re-run.")
        return False

# --- Main Function (ä¸å˜) ---
def main():
    bot_token = os.environ.get("BOT")
    url_list_repo_api = os.environ.get("URL_LIST_REPO_API")
    
    try:
        parts = url_list_repo_api.split('/')
        owner = parts[4]
        repo_name = parts[5]
        file_path_in_repo = '/'.join(parts[7:])
    except IndexError:
        print("Error: URL_LIST_REPO_API format is incorrect. Ensure it's a GitHub API file content link.")
        exit(1)

    repo_contents_api_base = f"https://api.github.com/repos/{owner}/{repo_name}/contents"

    if not bot_token or not url_list_repo_api:
        print("Error: Environment variables BOT or URL_LIST_REPO_API are not set!")
        print("Please ensure you've correctly set these variables in GitHub Actions secrets/variables.")
        exit(1)

    print("Fetching URL list and its SHA from GitHub...")
    url_content, url_file_sha = get_github_file_content(url_list_repo_api, bot_token)

    if url_content is None or url_file_sha is None:
        print("Could not get URL list or its SHA, script terminated.")
        exit(1)

    urls = url_content.strip().split('\n')
    print(f"Fetched {len(urls)} subscription URLs from GitHub.")

    # é‡ç‚¹ï¼šç°åœ¨ fetch_and_decode_urls_to_clash_proxies è¿”å›çš„æ˜¯ç»è¿‡å»é‡å’Œå‘½åæ ‡å‡†åŒ–çš„ä»£ç†å­—å…¸åˆ—è¡¨
    all_parsed_proxies, successful_urls = fetch_and_decode_urls_to_clash_proxies(urls)

    # æ„å»º Clash å®Œæ•´é…ç½®
    clash_config = {
        'port': 7890,
        'socks-port': 7891,
        'redir-port': 7892,
        'tproxy-port': 7893,
        'mixed-port': 7890,
        'mode': 'rule',
        'log-level': 'info',
        'allow-lan': True,
        'bind-address': '*',
        'external-controller': '127.0.0.1:9090',
        'dns': {
            'enable': True,
            'ipv6': False,
            'enhanced-mode': 'fake-ip',
            'listen': '0.0.0.0:53',
            'default-nameserver': [
                '114.114.114.114',
                '8.8.8.8'
            ],
            'nameserver': [
                'https://dns.google/dns-query',
                'tls://dns.google'
            ],
            'fallback': [
                'tls://1.1.1.1',
                'tcp://8.8.4.4',
                'https://dns.opendns.com/dns-query'
            ],
            'fallback-filter': {
                'geoip': True,
                'geoip-code': 'CN',
                'ipcidr': [
                    '240.0.0.0/4'
                ]
            ]
            }
        },
        'proxies': all_parsed_proxies, # å¡«å…¥æ‰€æœ‰è§£æå‡ºçš„ä»£ç†

        # ç¤ºä¾‹ä»£ç†ç»„ (å¯æ ¹æ®éœ€è¦è‡ªå®šä¹‰)
        'proxy-groups': [
            {
                'name': 'ğŸš€ èŠ‚ç‚¹é€‰æ‹©',
                'type': 'select',
                'proxies': ['DIRECT'] + [p['name'] for p in all_parsed_proxies] # æ·»åŠ DIRECTé€‰é¡¹
            },
            {
                'name': 'ğŸ“² å›½å¤–åª’ä½“',
                'type': 'select',
                'proxies': ['ğŸš€ èŠ‚ç‚¹é€‰æ‹©', 'DIRECT']
            },
            {
                'name': 'ğŸ¤– AI/ChatGPT',
                'type': 'select',
                'proxies': ['ğŸš€ èŠ‚ç‚¹é€‰æ‹©', 'DIRECT']
            },
            {
                'name': 'ğŸŒ å…¶ä»–æµé‡',
                'type': 'select',
                'proxies': ['ğŸš€ èŠ‚ç‚¹é€‰æ‹©', 'DIRECT']
            },
            {
                'name': 'ğŸŸ æ¼ç½‘ä¹‹é±¼',
                'type': 'select',
                'proxies': ['ğŸš€ èŠ‚ç‚¹é€‰æ‹©', 'DIRECT']
            },
            {
                'name': 'ğŸ›‘ å¹¿å‘Šæ‹¦æˆª',
                'type': 'select',
                'proxies': ['REJECT', 'DIRECT']
            },
            {
                'name': 'ğŸ”° Fallback',
                'type': 'fallback',
                'proxies': [p['name'] for p in all_parsed_proxies],
                'url': 'http://www.google.com/generate_204',
                'interval': 300
            }
        ],
        # ç¤ºä¾‹è§„åˆ™ (å¯æ ¹æ®éœ€è¦è‡ªå®šä¹‰)
        'rules': [
            'DOMAIN-KEYWORD,openai,ğŸ¤– AI/ChatGPT',
            'DOMAIN-KEYWORD,google,ğŸ“² å›½å¤–åª’ä½“',
            'DOMAIN-KEYWORD,youtube,ğŸ“² å›½å¤–åª’ä½“',
            'DOMAIN-KEYWORD,netflix,ğŸ“² å›½å¤–åª’ä½“',
            'DOMAIN-KEYWORD,github,ğŸŒ å…¶ä»–æµé‡', # GitHubä¹Ÿé€šè¿‡ä»£ç†ï¼Œé˜²æ­¢è¢«å¢™
            'DOMAIN-SUFFIX,cn,DIRECT',
            'IP-CIDR,172.16.0.0/12,DIRECT,no-resolve',
            'IP-CIDR,192.168.0.0/16,DIRECT,no-resolve',
            'IP-CIDR,10.0.0.0/8,DIRECT,no-resolve',
            'IP-CIDR,127.0.0.1/8,DIRECT,no-resolve',
            'GEOIP,CN,DIRECT,no-resolve',
            'MATCH,ğŸŸ æ¼ç½‘ä¹‹é±¼'
        ]
    }
    
    # å…³é”®è¡Œ: ä½¿ç”¨ default_flow_style=False é¼“åŠ±å•è¡Œè¾“å‡ºå­—å…¸
    final_clash_yaml = yaml.dump(clash_config, allow_unicode=True, sort_keys=False, default_flow_style=False, indent=2)

    # ç¼–ç ä¸ºBase64
    final_base64_encoded = base64.b64encode(final_clash_yaml.encode('utf-8')).decode('utf-8')

    with open("base64.txt", "w", encoding="utf-8") as f:
        f.write(final_base64_encoded)
    print("Base64 ç¼–ç çš„ Clash YAML é…ç½®å·²æˆåŠŸå†™å…¥ base64.txt")

    # ... (æ›´æ–° url.txt çš„é€»è¾‘ä¿æŒä¸å˜)
    new_url_list_content = "\n".join(sorted(list(set(successful_urls))))
    
    if new_url_list_content.strip() != url_content.strip():
        print("æ­£åœ¨æ›´æ–° GitHub ä¸Šçš„ url.txt æ–‡ä»¶...")
        commit_message = "feat: Update url.txt with valid subscription links (auto-filtered)"
        update_success = update_github_file_content(
            repo_contents_api_base,
            bot_token,
            file_path_in_repo,
            new_url_list_content,
            url_file_sha,
            commit_message
        )
        if update_success:
            print("url.txt æ–‡ä»¶å·²æˆåŠŸæ›´æ–°ã€‚")
        else:
            print("æ›´æ–° url.txt æ–‡ä»¶å¤±è´¥ã€‚")
    else:
        print("url.txt æ–‡ä»¶å†…å®¹æœªæ”¹å˜ï¼Œæ— éœ€æ›´æ–°ã€‚")

if __name__ == "__main__":
    main()
