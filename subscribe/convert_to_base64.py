import requests
import base64
import os
import json
import re
import yaml
from urllib.parse import urlparse, parse_qs, unquote
import hashlib
import socket
import time
import concurrent.futures # æ–°å¢å¯¼å…¥ï¼Œç”¨äºå¹¶è¡ŒåŒ–

# --- Proxy Parsing Functions ---
def generate_proxy_fingerprint(proxy_data):
    """
    æ ¹æ®ä»£ç†çš„å…³é”®è¿æ¥ä¿¡æ¯ç”Ÿæˆä¸€ä¸ªå”¯ä¸€çš„å“ˆå¸ŒæŒ‡çº¹ã€‚
    è¿™ç”¨äºè¯†åˆ«å’Œå»é‡ç›¸åŒçš„ä»£ç†ï¼Œå³ä½¿å®ƒä»¬çš„åç§°ä¸åŒã€‚
    """
    parts = []
    # å°½å¯èƒ½åŒ…å«æ‰€æœ‰æ ¸å¿ƒè¿æ¥å‚æ•°
    parts.append(proxy_data.get('type', ''))
    parts.append(str(proxy_data.get('server', '')))
    parts.append(str(proxy_data.get('port', '')))
    parts.append(str(proxy_data.get('uuid', '')))
    parts.append(str(proxy_data.get('password', '')))
    parts.append(str(proxy_data.get('cipher', '')))
    parts.append(str(proxy_data.get('network', '')))
    parts.append(str(proxy_data.get('tls', '')))
    parts.append(str(proxy_data.get('servername', '')))
    parts.append(str(proxy_data.get('ws-path', '')))
    parts.append(str(proxy_data.get('plugin-info', '')))
    parts.append(str(proxy_data.get('alpn', '')))

    unique_string = "_".join(parts)
    return hashlib.md5(unique_string.encode('utf-8')).hexdigest()

def parse_vmess(vmess_url):
    try:
        json_str = base64.b64decode(vmess_url[8:]).decode('utf-8')
        config = json.loads(json_str)

        name = config.get('ps', f"Vmess-{config.get('add')}")
        server = config.get('add')
        port = config.get('port')
        uuid = config.get('id')
        alterId = config.get('aid', 0)
        cipher = config.get('scy', 'auto')
        network = config.get('net', 'tcp')
        tls = config.get('tls', '') == 'tls'
        servername = config.get('sni', config.get('host', '')) if tls else ''
        skip_cert_verify = config.get('v', '') == '1'

        proxy = {
            'name': name,
            'type': 'vmess',
            'server': server,
            'port': port,
            'uuid': uuid,
            'alterId': alterId,
            'cipher': cipher,
            'network': network,
            'tls': tls,
        }

        if servername:
            proxy['servername'] = servername
        if skip_cert_verify:
            proxy['skip-cert-verify'] = True

        return proxy
    except Exception as e:
        print(f"è§£æ Vmess é“¾æ¥å¤±è´¥: {vmess_url[:50]}...ï¼ŒåŸå› : {e}")
        return None

def parse_trojan(trojan_url):
    try:
        parsed = urlparse(trojan_url)
        password = parsed.username
        server = parsed.hostname
        port = parsed.port
        name = unquote(parsed.fragment) if parsed.fragment else f"Trojan-{server}"

        params = parse_qs(parsed.query)
        tls = True
        skip_cert_verify = params.get('allowInsecure', ['0'])[0] == '1'
        servername = params.get('sni', [server])[0]

        proxy = {
            'name': name,
            'type': 'trojan',
            'server': server,
            'port': port,
            'password': password,
            'tls': tls,
        }
        if servername:
            proxy['servername'] = servername
        if skip_cert_verify:
            proxy['skip-cert-verify'] = True

        return proxy
    except Exception as e:
        print(f"è§£æ Trojan é“¾æ¥å¤±è´¥: {trojan_url[:50]}...ï¼ŒåŸå› : {e}")
        return None

def parse_shadowsocks(ss_url):
    try:
        encoded_part = ss_url[5:]

        name = "Shadowsocks"
        plugin_info_str = ""

        if '#' in encoded_part:
            encoded_part, fragment = encoded_part.split('#', 1)
            name = unquote(fragment)

        if '/?plugin=' in encoded_part:
            encoded_part, plugin_info_str = encoded_part.split('/?plugin=', 1)
            plugin_info_str = unquote(plugin_info_str)

        missing_padding = len(encoded_part) % 4
        if missing_padding:
            encoded_part += '=' * (4 - missing_padding)
            
        try:
            # å°è¯•ä½¿ç”¨ utf-8 è§£ç ï¼Œå¦‚æœå¤±è´¥åˆ™å°è¯• latin-1
            decoded_bytes = base64.urlsafe_b64decode(encoded_part)
            try:
                decoded_str = decoded_bytes.decode('utf-8')
            except UnicodeDecodeError:
                decoded_str = decoded_bytes.decode('latin-1', errors='ignore')
                print(f"    Warning: Shadowsocks link decoded to non-UTF-8 characters, using latin-1 for {ss_url[:50]}...")
                
            # å…³é”®ä¿®æ”¹ï¼šåªå–ç¬¬ä¸€ä¸ª '@' ä¹‹å‰å’Œä¹‹åçš„éƒ¨åˆ†ï¼Œå¿½ç•¥æ‰€æœ‰åç»­å†…å®¹
            parts = decoded_str.split('@', 1) # åªåˆ†å‰²ä¸€æ¬¡
            
            if len(parts) != 2:
                raise ValueError(f"Invalid format after base64 decoding: Missing '@' separator or incorrect structure.")

            method_password = parts[0]
            server_port_and_tail = parts[1] # åŒ…å«æœåŠ¡å™¨å’Œç«¯å£ï¼Œä»¥åŠå¯èƒ½å­˜åœ¨çš„åç»­ä¹±ç 

            # è¿›ä¸€æ­¥æ¸…ç† server_port_and_tailï¼Œåªä¿ç•™æœ‰æ•ˆçš„æœåŠ¡å™¨:ç«¯å£éƒ¨åˆ†
            # æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…ï¼š
            # ^                   - å­—ç¬¦ä¸²å¼€å¤´
            # [\w\d\.\-]+         - åŒ¹é…ä¸€ä¸ªæˆ–å¤šä¸ªå•è¯å­—ç¬¦ï¼ˆå­—æ¯ã€æ•°å­—ã€ä¸‹åˆ’çº¿ï¼‰ã€æ•°å­—ã€ç‚¹æˆ–è¿å­—ç¬¦ï¼ˆç”¨äºæœåŠ¡å™¨åï¼‰
            # :                   - åŒ¹é…å†’å·
            # \d+                 - åŒ¹é…ä¸€ä¸ªæˆ–å¤šä¸ªæ•°å­—ï¼ˆç«¯å£å·ï¼‰
            clean_server_port_match = re.match(r'^[\w\d\.\-]+\:\d+', server_port_and_tail)
            if clean_server_port_match:
                server_port_str = clean_server_port_match.group(0)
            else:
                raise ValueError(f"Invalid server:port format in: '{server_port_and_tail}'")

            method_password_parts = method_password.split(':', 1)
            if len(method_password_parts) != 2:
                raise ValueError(f"Invalid method:password format: '{method_password}'")
            method = method_password_parts[0]
            password = method_password_parts[1]

            server_port_parts = server_port_str.split(':')
            if len(server_port_parts) != 2:
                raise ValueError(f"Invalid server:port format: '{server_port_str}'")
            server = server_port_parts[0]
            port = int(server_port_parts[1])

            proxy = {
                'name': name,
                'type': 'ss',
                'server': server,
                'port': port,
                'cipher': method,
                'password': password,
            }

            if plugin_info_str:
                proxy['plugin-info'] = plugin_info_str

            return proxy
        except (base64.binascii.Error) as b64_err:
            raise ValueError(f"Base64 decoding error: {b64_err}")
    except Exception as e:
        print(f"è§£æ Shadowsocks é“¾æ¥å¤±è´¥: {ss_url[:100]}...ï¼ŒåŸå› : {e}")
        return None

def parse_hysteria2(hy2_url):
    try:
        parsed = urlparse(hy2_url)
        uuid = parsed.username
        server = parsed.hostname
        port = parsed.port
        name = unquote(parsed.fragment) if parsed.fragment else f"Hysteria2-{server}"

        params = parse_qs(parsed.query)
        tls = params.get('security', [''])[0].lower() == 'tls'
        servername = params.get('sni', [''])[0]
        skip_cert_verify = params.get('insecure', ['0'])[0] == '1'
        fast_open = params.get('fastopen', ['0'])[0] == '1'

        proxy = {
            'name': name,
            'type': 'hysteria2',
            'server': server,
            'port': port,
            'password': uuid,
            'tls': tls,
            'skip-cert-verify': skip_cert_verify,
            'fast-open': fast_open,
        }
        if servername:
            proxy['servername'] = servername
        if params.get('alpn'):
            proxy['alpn'] = ','.join(params['alpn'])

        return proxy
    except Exception as e:
        print(f"è§£æ Hysteria2 é“¾æ¥å¤±è´¥: {hy2_url[:50]}...ï¼ŒåŸå› : {e}")
        return None

# --- Connectivity Test Function ---
def test_tcp_connectivity(server, port, timeout=1, retries=1, delay=0.5):
    """
    å°è¯•ä¸æŒ‡å®šçš„æœåŠ¡å™¨å’Œç«¯å£å»ºç«‹TCPè¿æ¥ï¼Œæµ‹è¯•è¿é€šæ€§ã€‚
    å¢åŠ é‡è¯•æœºåˆ¶ï¼Œä»¥åº”å¯¹ç¬æ—¶ç½‘ç»œæŠ–åŠ¨æˆ–æœåŠ¡å™¨çŸ­æš‚é—®é¢˜ã€‚
    è¿”å› True å¦‚æœè¿æ¥æˆåŠŸï¼Œå¦åˆ™è¿”å› Falseã€‚
    **å‚æ•°å·²è°ƒæ•´ä¸ºæ›´å¿«çš„å¤±è´¥ç­–ç•¥ã€‚**
    """
    for i in range(retries + 1): # retries + 1 = ç¬¬ä¸€æ¬¡å°è¯• + é‡è¯•æ¬¡æ•°
        try:
            sock = socket.create_connection((server, port), timeout=timeout)
            sock.close()
            return True
        except (socket.timeout, ConnectionRefusedError, OSError) as e:
            # print(f"    è¿æ¥å°è¯• {i+1}/{retries+1} å¤±è´¥ for {server}:{port} - {e}") # å‡å°‘æ—¥å¿—è¾“å‡ºï¼Œé¿å…å¹¶è¡ŒåŒ–æ—¶æ—¥å¿—è¿‡å¤š
            if i < retries: # å¦‚æœä¸æ˜¯æœ€åä¸€æ¬¡å°è¯•ï¼Œåˆ™ç­‰å¾…å¹¶é‡è¯•
                time.sleep(delay)
        except Exception as e:
            # print(f"    TCPè¿æ¥æµ‹è¯•å‘ç”ŸæœªçŸ¥é”™è¯¯: {server}:{port} - {e}") # å‡å°‘æ—¥å¿—è¾“å‡º
            return False
    return False # æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥

# --- Fetch and Decode URLs (Modified for deduplication and naming) ---
def fetch_and_decode_urls_to_clash_proxies(urls, enable_connectivity_test=True):
    all_raw_proxies = [] # æ”¶é›†æ‰€æœ‰è§£æå‡ºçš„ä»£ç†ï¼ˆåŒ…å«é‡å¤çš„ï¼‰
    successful_urls = set()

    EXCLUDE_KEYWORDS = [
        "cdn.jsdelivr.net", "statically.io", "googletagmanager.com",
        "www.w3.org", "fonts.googleapis.com", "schemes.ogf.org", "clashsub.net",
        "t.me", "api.w.org",
        "html", "css", "js", "ico", "png", "jpg", "jpeg", "gif", "svg", "webp", "xml", "json", "txt",
        "google-analytics.com", "cloudflare.com/cdn-cgi/", "gstatic.com", "googleapis.com",
        "disqus.com", "gravatar.com", "s.w.org",
        "amazon.com", "aliyuncs.com", "tencentcos.cn",
        "cdn.bootcss.com", "cdnjs.cloudflare.com",
        "bit.ly", "tinyurl.com", "cutt.ly", "shorturl.at", "surl.li", "suo.yt", "v1.mk",
        "youtube.com", "facebook.com", "twitter.com", "weibo.com",
        "mail.google.com", "docs.google.com",
        "microsoft.com", "apple.com", "baidu.com", "qq.com",
        ".woff", ".woff2", ".ttf", ".otf", ".eot",
        ".zip", ".rar", ".7z", ".tar.gz", ".exe", ".dmg", ".apk",
        "/assets/", "/static/", "/images/", "/scripts/", "/styles/", "/fonts/",
        "robots.txt", "sitemap.xml", "favicon.ico",
        "rss", "atom",
        "/LICENSE", "/README.md", "/CHANGELOG.md",
        ".git", ".svn",
        "swagger-ui.html", "openapi.json"
    ]

    for url in urls:
        url = url.strip()
        if not url:
            continue

        if any(keyword in url for keyword in EXCLUDE_KEYWORDS):
            print(f"Skipping non-subscription link (filtered by keyword): {url}")
            continue

        print(f"Processing URL: {url}")
        current_proxies_from_url = [] # å­˜å‚¨å½“å‰URLè§£æå‡ºçš„ä»£ç†
        try:
            response = requests.get(url, timeout=20)
            response.raise_for_status()
            content = response.content

            print(f"  --- URL: {url} Downloaded content size: {len(content)} bytes ---")

            def try_parse_yaml(text):
                try:
                    data = yaml.safe_load(text)
                    if isinstance(data, dict) and 'proxies' in data and isinstance(data['proxies'], list):
                        return data['proxies']
                    elif isinstance(data, list) and all(isinstance(item, dict) and 'type' in item for item in data):
                        return data
                    return None
                except yaml.YAMLError:
                    return None

            def try_parse_json_nodes(text):
                try:
                    data = json.loads(text)
                    if isinstance(data, list) and all(isinstance(item, dict) and 'v' in item for item in data):
                        # å¯¹äº V2rayN å¯¼å‡ºçš„ JSON æ ¼å¼
                        parsed_list = []
                        for node in data:
                            vmess_link = f"vmess://{base64.b64encode(json.dumps(node).encode('utf-8')).decode('utf-8')}"
                            p = parse_vmess(vmess_link)
                            if p: parsed_list.append(p)
                        return parsed_list
                    return None
                except json.JSONDecodeError:
                    return None

            try:
                decoded_content = content.decode('utf-8')
                # print(f"  --- URL: {url} Successfully decoded to UTF-8 ---") # å‡å°‘æ—¥å¿—è¾“å‡º

                yaml_proxies = try_parse_yaml(decoded_content)
                if yaml_proxies:
                    current_proxies_from_url.extend(yaml_proxies)
                    print(f"  --- URL: {url} Identified as YAML subscription ---")
                else:
                    json_proxies = try_parse_json_nodes(decoded_content)
                    if json_proxies:
                        current_proxies_from_url.extend(json_proxies)
                        print(f"  --- URL: {url} Identified as JSON node list ---")
                    else:
                        # å°è¯• Base64 è§£ç ï¼Œå³ä½¿æ˜¯ UTF-8 ä¹Ÿèƒ½å¤„ç† Base64 ç¼–ç çš„è®¢é˜…
                        if len(decoded_content.strip()) > 0 and len(decoded_content.strip()) % 4 == 0 and re.fullmatch(r'[A-Za-z0-9+/=]*', decoded_content.strip()):
                            try:
                                temp_decoded = base64.b64decode(decoded_content.strip()).decode('utf-8')
                                lines = temp_decoded.split('\n')
                                parsed_line_count = 0
                                for line in lines:
                                    line = line.strip()
                                    if line.startswith("vmess://"):
                                        p = parse_vmess(line)
                                        if p: current_proxies_from_url.append(p); parsed_line_count += 1
                                    elif line.startswith("trojan://"):
                                        p = parse_trojan(line)
                                        if p: current_proxies_from_url.append(p); parsed_line_count += 1
                                    elif line.startswith("ss://"):
                                        p = parse_shadowsocks(line)
                                        if p: current_proxies_from_url.append(p); parsed_line_count += 1
                                    elif line.startswith("hysteria2://"):
                                        p = parse_hysteria2(line)
                                        if p: current_proxies_from_url.append(p); parsed_line_count += 1
                                if parsed_line_count > 0:
                                    print(f"  --- URL: {url} Base64 decoded and identified {parsed_line_count} proxy nodes ---")
                                else:
                                    print(f"  --- URL: {url} Base64 decoded successfully, but content doesn't match known proxy format.---")
                            except (base64.binascii.Error, UnicodeDecodeError):
                                print(f"  --- URL: {url} Looks like Base64 but failed to decode, treating as plaintext.---")

                        if not current_proxies_from_url: # å¦‚æœ Base64 è§£ç æˆ–ä¹‹å‰çš„è§£ææ²¡æœ‰å¾—åˆ°ä»£ç†ï¼Œå°è¯•ç›´æ¥è§£æä¸ºçº¯æ–‡æœ¬é“¾æ¥
                            lines = decoded_content.split('\n')
                            parsed_line_count = 0
                            for line in lines:
                                line = line.strip()
                                if line.startswith("vmess://"):
                                    p = parse_vmess(line)
                                    if p: current_proxies_from_url.append(p); parsed_line_count += 1
                                elif line.startswith("trojan://"):
                                    p = parse_trojan(line)
                                    if p: current_proxies_from_url.append(p); parsed_line_count += 1
                                elif line.startswith("ss://"):
                                    p = parse_shadowsocks(line)
                                    if p: current_proxies_from_url.append(p); parsed_line_count += 1
                                elif line.startswith("hysteria2://"):
                                    p = parse_hysteria2(line)
                                    if p: current_proxies_from_url.append(p); parsed_line_count += 1
                            if parsed_line_count > 0:
                                print(f"  --- URL: {url} Identified as plaintext {parsed_line_count} proxy nodes ---")
                            else:
                                print(f"  --- URL: {url} Content not identified as valid subscription format (UTF-8).---")

            except UnicodeDecodeError: # å¦‚æœç›´æ¥ UTF-8 è§£ç å¤±è´¥ï¼Œå†å°è¯• Base64 è§£ç 
                print(f"  --- URL: {url} UTF-8 decoding failed, trying Base64 decoding.---")
                try:
                    cleaned_content = content.strip()
                    temp_decoded = base64.b64decode(cleaned_content).decode('utf-8')
                    # print(f"  --- URL: {url} Successfully decoded Base64 content to UTF-8 ---") # å‡å°‘æ—¥å¿—è¾“å‡º

                    yaml_proxies = try_parse_yaml(temp_decoded)
                    if yaml_proxies:
                        current_proxies_from_url.extend(yaml_proxies)
                        print(f"  --- URL: {url} Base64 decoded to YAML subscription ---")
                    else:
                        json_proxies = try_parse_json_nodes(temp_decoded)
                        if json_proxies:
                            current_proxies_from_url.extend(json_proxies)
                            print(f"  --- URL: {url} Base64 decoded to JSON node list ---")
                        else:
                            lines = temp_decoded.split('\n')
                            parsed_line_count = 0
                            for line in lines:
                                line = line.strip()
                                if line.startswith("vmess://"):
                                    p = parse_vmess(line)
                                    if p: current_proxies_from_url.append(p); parsed_line_count += 1
                                elif line.startswith("trojan://"):
                                    p = parse_trojan(line)
                                    if p: current_proxies_from_url.append(p); parsed_line_count += 1
                                elif line.startswith("ss://"):
                                    p = parse_shadowsocks(line)
                                    if p: current_proxies_from_url.append(p); parsed_line_count += 1
                                elif line.startswith("hysteria2://"):
                                    p = parse_hysteria2(line)
                                    if p: current_proxies_from_url.append(p); parsed_line_count += 1
                            if parsed_line_count > 0:
                                print(f"  --- URL: {url} Base64 decoded to {parsed_line_count} proxy nodes ---")
                            else:
                                print(f"  --- URL: {url} Base64 decoded successfully, but content doesn't match known proxy format.---")

                except (base64.binascii.Error, UnicodeDecodeError) as decode_err:
                    print(f"  --- URL: {url} Base64 decoding or UTF-8 conversion failed: {decode_err} ---")
                    # Fallback to latin-1 for content that can't be decoded to UTF-8 or Base64 (for logging purposes)
                    content.decode('latin-1', errors='ignore')  
                    print(f"Warning: Could not decode content from {url} to UTF-8 or Base64. Using latin-1 and ignoring errors.")

            # å°†å½“å‰URLè§£æå‡ºçš„ä»£ç†æ·»åŠ åˆ°æ€»åˆ—è¡¨
            if current_proxies_from_url:
                all_raw_proxies.extend(current_proxies_from_url)
                successful_urls.add(url)

        except requests.exceptions.RequestException as e:
            print(f"Failed to fetch data from URL: {url}, reason: {e}")
        except Exception as e:
            print(f"An unexpected error occurred while processing URL {url}: {e}")

    # --- å»é‡å’Œè¿é€šæ€§æµ‹è¯• (å¹¶è¡ŒåŒ–) ---
    unique_proxies_for_test = {}
    for proxy_dict in all_raw_proxies:
        if proxy_dict:
            fingerprint = generate_proxy_fingerprint(proxy_dict)
            if fingerprint not in unique_proxies_for_test:
                unique_proxies_for_test[fingerprint] = proxy_dict
            
    proxies_to_test_list = list(unique_proxies_for_test.values())
    final_filtered_proxies = []

    if enable_connectivity_test:
        print(f"\nå¼€å§‹å¹¶è¡Œè¿é€šæ€§æµ‹è¯•ï¼Œå…± {len(proxies_to_test_list)} ä¸ªå”¯ä¸€ä»£ç†...")
        # çº¿ç¨‹æ± å·¥ä½œçº¿ç¨‹æ•°é‡ï¼Œæ ¹æ®å®é™…GitHub Actionsçš„CPUå’Œå†…å­˜é™åˆ¶è°ƒæ•´
        # é€šå¸¸20-50ä¸ªçº¿ç¨‹æ˜¯æ¯”è¾ƒåˆç†çš„èŒƒå›´
        with concurrent.futures.ThreadPoolExecutor(max_workers=30) as executor:
            # æäº¤æ‰€æœ‰ä»£ç†æµ‹è¯•ä»»åŠ¡
            # future_to_proxy æ˜ å°„ future å¯¹è±¡åˆ°åŸå§‹çš„ proxy_dict
            future_to_proxy = {
                executor.submit(test_tcp_connectivity, p['server'], p['port']): p
                for p in proxies_to_test_list if p.get('server') and isinstance(p.get('port'), int)
            }

            processed_count = 0
            total_testable_proxies = len(future_to_proxy) # å®é™…æäº¤æµ‹è¯•çš„ä»£ç†æ•°é‡

            for future in concurrent.futures.as_completed(future_to_proxy):
                proxy_dict = future_to_proxy[future]
                server = proxy_dict.get('server')
                port = proxy_dict.get('port')
                processed_count += 1

                try:
                    is_reachable = future.result()
                    if is_reachable:
                        # åªæœ‰åœ¨æµ‹è¯•é€šè¿‡åæ‰ç”Ÿæˆæœ€ç»ˆçš„åç§°å’Œæ·»åŠ åˆ°åˆ—è¡¨
                        base_name = f"{proxy_dict.get('type', 'unknown').upper()}-{proxy_dict.get('server', 'unknown')}"
                        proxy_dict['name'] = f"{base_name}-{generate_proxy_fingerprint(proxy_dict)[:8]}"
                        final_filtered_proxies.append(proxy_dict)
                        # print(f"    èŠ‚ç‚¹å¯è¾¾ï¼Œå·²æ·»åŠ : {proxy_dict['name']}") # å¹¶è¡Œæ—¶å‡å°‘è¾“å‡º
                    else:
                        pass # print(f"    èŠ‚ç‚¹ä¸å¯è¾¾ï¼ˆå¤šæ¬¡å°è¯•åï¼‰ï¼Œå·²è·³è¿‡: {server}:{port}") # å¹¶è¡Œæ—¶å‡å°‘è¾“å‡º
                except Exception as exc:
                    print(f"    è¿é€šæ€§æµ‹è¯• {server}:{port} æ—¶å‘ç”Ÿå¼‚å¸¸: {exc}")
                
                # æ‰“å°è¿›åº¦ (æ¯50ä¸ªä»£ç†æ‰“å°ä¸€æ¬¡ï¼Œæˆ–æœ€åä¸€æ¬¡)
                if processed_count % 50 == 0 or processed_count == total_testable_proxies:
                    print(f"    è¿›åº¦: å·²æµ‹è¯• {processed_count}/{total_testable_proxies} ä¸ªä»£ç†...")

    else:
        print("è·³è¿‡è¿é€šæ€§æµ‹è¯• (å·²ç¦ç”¨)ã€‚æ‰€æœ‰è§£æå‡ºçš„å”¯ä¸€ä»£ç†å°†è¢«æ·»åŠ ã€‚")
        for proxy_dict in proxies_to_test_list:
            base_name = f"{proxy_dict.get('type', 'unknown').upper()}-{proxy_dict.get('server', 'unknown')}"
            proxy_dict['name'] = f"{base_name}-{generate_proxy_fingerprint(proxy_dict)[:8]}"
            final_filtered_proxies.append(proxy_dict)

    print(f"Successfully parsed, deduplicated, tested, and aggregated {len(final_filtered_proxies)} unique and reachable proxy nodes.")
    return final_filtered_proxies, list(successful_urls)

# --- GitHub API Helpers (Modified to check ETag if X-GitHub-Sha is missing) ---
def get_github_file_content(api_url, token):
    headers = {"Authorization": f"token {token}", "Accept": "application/vnd.github.v3.raw"}
    try:
        print(f"DEBUG: å°è¯•ä» GitHub API è·å–æ–‡ä»¶: {api_url}")
        
        response = requests.get(api_url, headers=headers, timeout=10)
        
        # æ‰“å°æ‰€æœ‰å“åº”å¤´
        # print("DEBUG: GitHub API å“åº”å¤´:")
        # for header, value in response.headers.items():
        #     print(f"  {header}: {value}")
            
        print(f"DEBUG: GitHub API å“åº”çŠ¶æ€ç : {response.status_code}")
        
        # ä¼˜å…ˆä» X-GitHub-Sha è·å– SHAï¼Œå¦‚æœä¸å­˜åœ¨ï¼Œåˆ™å°è¯•ä» ETag è·å–
        sha = response.headers.get("X-GitHub-Sha")
        if sha is None:
            etag = response.headers.get("ETag")
            if etag:
                # ETag å€¼é€šå¸¸æ˜¯å¸¦å¼•å·çš„ï¼Œæˆ‘ä»¬éœ€è¦å»é™¤å¼•å·
                sha = etag.strip('"')
                print(f"DEBUG: X-GitHub-Sha ä¸º Noneï¼Œä» ETag è·å–åˆ° SHA: {sha}")
            else:
                print("DEBUG: æ—¢æœªè·å–åˆ° X-GitHub-Shaï¼Œä¹Ÿæœªè·å–åˆ° ETagã€‚")
        else:
            print(f"DEBUG: ä» X-GitHub-Sha è·å–åˆ° SHA: {sha}")
        
        response.raise_for_status()
        
        # print("DEBUG: GitHub API å“åº”å†…å®¹ç‰‡æ®µ (å‰500å­—ç¬¦):") # å‡å°‘æ—¥å¿—è¾“å‡º
        # print(response.text[:500])
        
        return response.text, sha
    except requests.exceptions.HTTPError as http_err:
        print(f"Error fetching file from GitHub (HTTP Error): {http_err}")
        if response is not None:
            print(f"DEBUG: é”™è¯¯å“åº”å†…å®¹: {response.text}")
        return None, None
    except requests.exceptions.RequestException as req_err:
        print(f"Error fetching file from GitHub (Request Error): {req_err}")
        return None, None
    except Exception as e:
        print(f"Error fetching file from GitHub (Other Error): {e}")
        return None, None

def update_github_file_content(repo_contents_api_base, token, file_path, new_content, sha, commit_message):
    url = f"{repo_contents_api_base}/{file_path}"
    headers = {
        "Authorization": f"token {token}",
        "Accept": "application/vnd.github.v3+json",
        "Content-Type": "application/json"
    }
    data = {
        "message": commit_message,
        "content": base64.b64encode(new_content.encode('utf-8')).decode('utf-8'),
        "sha": sha
    }
    try:
        response = requests.put(url, headers=headers, data=json.dumps(data), timeout=10)
        response.raise_for_status()
        return True
    except requests.exceptions.RequestException as e:
        print(f"Error updating file on GitHub: {e}")
        if response and response.status_code == 409:
            print("Conflict: File content changed on GitHub before commit. Please re-run.")
        return False

# --- Main Function ---
def main():
    bot_token = os.environ.get("BOT")
    url_list_repo_api = os.environ.get("URL_LIST_REPO_API")

    try:
        # ç¡®ä¿ url_list_repo_api æ˜¯ä¸€ä¸ªå®Œæ•´çš„ GitHub Content API URL
        # ä¾‹å¦‚: https://api.github.com/repos/owner/repo/contents/path/to/file.txt
        parts = url_list_repo_api.split('/')
        if len(parts) < 8 or parts[2] != 'api.github.com' or parts[3] != 'repos' or parts[6] != 'contents':
            raise ValueError("URL_LIST_REPO_API does not seem to be a valid GitHub Content API URL.")
            
        owner = parts[4]
        repo_name = parts[5]
        file_path_in_repo = '/'.join(parts[7:]) # 'contents' ä¹‹åçš„è·¯å¾„
    except ValueError as ve:
        print(f"Error: {ve}")
        print("Please ensure URL_LIST_REPO_API is correctly set to a GitHub Content API URL (e.g., https://api.github.com/repos/user/repo/contents/path/to/file.txt).")
        exit(1)
    except IndexError:
        print("Error: URL_LIST_REPO_API format is incorrect or incomplete. Cannot extract owner, repo, or file path.")
        exit(1)

    repo_contents_api_base = f"https://api.github.com/repos/{owner}/{repo_name}/contents"

    if not bot_token or not url_list_repo_api:
        print("Error: Environment variables BOT or URL_LIST_REPO_API are not set!")
        print("Please ensure you've correctly set these variables in GitHub Actions secrets/variables.")
        exit(1)

    print("Fetching URL list and its SHA from GitHub...")
    url_content, url_file_sha = get_github_file_content(url_list_repo_api, bot_token)

    if url_content is None or url_file_sha is None:
        print("Could not get URL list or its SHA, script terminated.")
        exit(1)

    urls = url_content.strip().split('\n')
    print(f"Fetched {len(urls)} subscription URLs from GitHub.")

    enable_connectivity_test = os.environ.get("ENABLE_CONNECTIVITY_TEST", "true").lower() == "true"


    all_parsed_proxies, successful_urls = fetch_and_decode_urls_to_clash_proxies(urls, enable_connectivity_test)

    # æ„å»º Clash å®Œæ•´é…ç½®
    clash_config = {
        'port': 7890,
        'socks-port': 7891,
        'redir-port': 7892,
        'tproxy-port': 7893,
        'mixed-port': 7890,
        'mode': 'rule',
        'log-level': 'info',
        'allow-lan': True,
        'bind-address': '*',
        'external-controller': '127.0.0.1:9090',
        'dns': {
            'enable': True,
            'ipv6': False,
            'enhanced-mode': 'fake-ip',
            'listen': '0.0.0.0:53',
            'default-nameserver': [
                '114.114.114.114',
                '8.8.8.8'
            ],
            'nameserver': [
                'https://dns.google/dns-query',
                'tls://dns.google'
            ],
            'fallback': [
                'tls://1.1.1.1',
                'tcp://8.8.4.4',
                'https://dns.opendns.com/dns-query'
            ],
            'fallback-filter': {
                'geoip': True,
                'geoip-code': 'CN',
                'ipcidr': [
                    '240.0.0.0/4'
                ]
            }
        },
        'proxies': all_parsed_proxies,

        'proxy-groups': [
            {
                'name': 'ğŸš€ èŠ‚ç‚¹é€‰æ‹©',
                'type': 'select',
                'proxies': ['DIRECT'] + [p['name'] for p in all_parsed_proxies]
            },
            {
                'name': 'ğŸ“² å›½å¤–åª’ä½“',
                'type': 'select',
                'proxies': ['ğŸš€ èŠ‚ç‚¹é€‰æ‹©', 'DIRECT']
            },
            {
                'name': 'ğŸ¤– AI/ChatGPT',
                'type': 'select',
                'proxies': ['ğŸš€ èŠ‚ç‚¹é€‰æ‹©', 'DIRECT']
            },
            {
                'name': 'ğŸŒ å…¶ä»–æµé‡',
                'type': 'select',
                'proxies': ['ğŸš€ èŠ‚ç‚¹é€‰æ‹©', 'DIRECT']
            },
            {
                'name': 'ğŸŸ æ¼ç½‘ä¹‹é±¼',
                'type': 'select',
                'proxies': ['ğŸš€ èŠ‚ç‚¹é€‰æ‹©', 'DIRECT']
            },
            {
                'name': 'ğŸ›‘ å¹¿å‘Šæ‹¦æˆª',
                'type': 'select',
                'proxies': ['REJECT', 'DIRECT']
            },
            {
                'name': 'ğŸ”° Fallback',
                'type': 'fallback',
                'proxies': [p['name'] for p in all_parsed_proxies],
                'url': 'http://www.google.com/generate_204',
                'interval': 300
            }
        ],
        'rules': [
            'DOMAIN-KEYWORD,openai,ğŸ¤– AI/ChatGPT',
            'DOMAIN-KEYWORD,google,ğŸ“² å›½å¤–åª’ä½“',
            'DOMAIN-KEYWORD,youtube,ğŸ“² å›½å¤–åª’ä½“',
            'DOMAIN-KEYWORD,netflix,ğŸ“² å›½å¤–åª’ä½“',
            'DOMAIN-KEYWORD,github,ğŸŒ å…¶ä»–æµé‡',
            'DOMAIN-SUFFIX,cn,DIRECT',
            'IP-CIDR,172.16.0.0/12,DIRECT,no-resolve',
            'IP-CIDR,192.168.0.0/16,DIRECT,no-resolve',
            'IP-CIDR,10.0.0.0/8,DIRECT,no-resolve',
            'IP-CIDR,127.0.0.1/8,DIRECT,no-resolve',
            'GEOIP,CN,DIRECT,no-resolve',
            'MATCH,ğŸŸ æ¼ç½‘ä¹‹é±¼'
        ]
    }

    # --- ç”ŸæˆåŸå§‹ YAML æ–‡ä»¶ ---
    final_clash_yaml = yaml.dump(clash_config, allow_unicode=True, sort_keys=False, default_flow_style=False, indent=2)
    with open("base64.yaml", "w", encoding="utf-8") as f:
        f.write(final_clash_yaml)
    print("Clash YAML configuration successfully written to base64.yaml")

    # --- ç”Ÿæˆ Base64 ç¼–ç åçš„æ–‡æœ¬æ–‡ä»¶ ---
    final_base64_encoded = base64.b64encode(final_clash_yaml.encode('utf-8')).decode('utf-8')
    with open("base64.txt", "w", encoding="utf-8") as f:
        f.write(final_base64_encoded)
    print("Base64 encoded Clash YAML configuration successfully written to base64.txt")

    new_url_list_content = "\n".join(sorted(list(set(successful_urls))))

    if new_url_list_content.strip() != url_content.strip():
        print("Updating GitHub url.txt file...")
        commit_message = "feat: Update url.txt with valid subscription links (auto-filtered)"
        update_success = update_github_file_content(
            repo_contents_api_base,
            bot_token,
            file_path_in_repo,
            new_url_list_content,
            url_file_sha,
            commit_message
        )
        if update_success:
            print("url.txt file updated successfully.")
        else:
            print("Failed to update url.txt file.")
    else:
        print("url.txt file content unchanged, no update needed.")

if __name__ == "__main__":
    main()
