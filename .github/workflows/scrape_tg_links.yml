name: Run Web Crawler

on:
  workflow_dispatch:
  push:
    branches: [ "main" ]
  schedule:
    - cron: '0 0 * * *' # 每天 UTC 00:00 运行

jobs:
  crawl:
    runs-on: ubuntu-latest

    steps:
    - name: Checkout repository
      uses: actions/checkout@v4

    - name: Set up Python 3.x
      uses: actions/setup-python@v5
      with:
        python-version: '3.x'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install selenium webdriver-manager

    - name: Set up data directory
      run: |
        mkdir -p data

    - name: Run web crawler
      env:
        BASE_URL: ${{ secrets.BASE_URL }}
        DATA_DIR: ${{ github.workspace }}/data
        OUTPUT_VALID_FILE: valid_links.txt
        OUTPUT_INVALID_FILE: invalid_links.txt
        MAX_PAGES: ${{ secrets.MAX_PAGES }}
        MAX_WORKERS: ${{ secrets.MAX_WORKERS }}
      run: python ${{ github.workspace }}/scrape_tg_links.py

    - name: Commit and push changes
      run: |
        git config --local user.email "github-actions[bot]@users.noreply.github.com"
        git config --local user.name "GitHub Actions Bot"
        git add data/valid_links.txt data/invalid_links.txt
        git commit -m "Add crawled links [skip ci]" || echo "No changes to commit"
        git push origin main
