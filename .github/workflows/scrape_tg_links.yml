name: Run Web Crawler

on:
  push:
    branches: [ "main" ]
  schedule:
    - cron: '0 0 * * *' # 每天 UTC 00:00 运行

jobs:
  crawl:
    runs-on: ubuntu-latest

    steps:
    - name: Checkout repository
      uses: actions/checkout@v4

    - name: Set up Python 3.x
      uses: actions/setup-python@v5
      with:
        python-version: '3.x'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt

    - name: Set up data directory
      run: |
        mkdir -p data

    - name: Run web crawler
      env:
        BASE_URL: ${{ secrets.BASE_URL }} # 从 Secrets 获取 BASE_URL
        DATA_DIR: ${{ github.workspace }}/data # 设置 DATA_DIR 环境变量
        OUTPUT_VALID_FILE: valid_links.txt # 从 config.ini 读取，或使用默认值
        OUTPUT_INVALID_FILE: invalid_links.txt # 从 config.ini 读取，或使用默认值
        MAX_PAGES: ${{ secrets.MAX_PAGES }} # 从 Secrets 获取 MAX_PAGES
        MAX_WORKERS: ${{ secrets.MAX_WORKERS }} # 从 Secrets 获取 MAX_WORKERS
      run: python ${{ github.workspace }}/scrape_tg_links.py

    - name: Upload valid links
      uses: actions/upload-artifact@v4
      with:
        name: valid-links
        path: ${{ github.workspace }}/data/valid_links.txt

    - name: Upload invalid links
      if: always() # 即使任务失败也上传
      uses: actions/upload-artifact@v4
      with:
        name: invalid-links
        path: ${{ github.workspace }}/data/invalid_links.txt
